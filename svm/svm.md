# 支持向量机（SVM）
支持向量机(Support Vector Machine，SVM)是Corinna Cortes和Vapnik等于1995年首先提出的，它在解决小样本、非线性及高维模式识别中表现出许多特有的优势，并能够推广应用到函数拟合等其他机器学习问题中。
在机器学习中，支持向量机（SVM，还支持矢量网络）是与相关的学习算法有关的监督学习模型，可以分析数据，识别模式，用于分类和回归分析。

支持向量机方法是建立在统计学习理论的VC维理论和结构风险最小原理基础上的，根据有限的样本信息在模型的复杂性（即对特定训练样本的学习精度）和学习能力（即无错误地识别任意样本的能力）之间寻求最佳折中，以求获得最好的推广能力 。

在机器学习中，支持向量机（SVM，还支持矢量网络）是与相关的学习算法有关的监督学习模型，可以分析数据，识别模式，用于分类和回归分析。给定一组训练样本，每个标记为属于两类，一个SVM训练算法建立了一个模型，分配新的实例为一类或其他类，使其成为非概率二元线性分类。一个SVM模型的例子，如在空间中的点，映射，使得所述不同的类别的例子是由一个明显的差距是尽可能宽划分的表示。新的实施例则映射到相同的空间中，并预测基于它们落在所述间隙侧上属于一个类别。
除了进行线性分类，支持向量机可以使用所谓的核技巧，它们的输入隐含映射成高维特征空间中有效地进行非线性分类。

更正式地说，一个支持向量机的构造一个超平面，或在高或无限维空间，其可以用于分类，回归，或其它任务中设定的超平面的。直观地，一个良好的分离通过具有到任何类（所谓官能余量）的最接近的训练数据点的最大距离的超平面的一般实现中，由于较大的裕度下分类器的泛化误差。
而原来的问题可能在一个有限维空间中所述，经常发生以鉴别集是不是在该空间线性可分。出于这个原因，有人建议，在原始有限维空间映射到一个高得多的立体空间，推测使分离在空间比较容易。保持计算负荷合理，使用支持向量机计划的映射被设计成确保在点积可在原空间中的变量而言容易地计算，通过定义它们中选择的核函数k（x，y）的计算以适应的问题。
在高维空间中的超平面被定义为一组点的点积与该空间中的向量是恒定的。限定的超平面的载体可被选择为线性组合与参数\alpha_i中发生的数据的基础上的特征向量的图像。这种选择一个超平面，该点中的x的特征空间映射到超平面是由关系定义：\字型\sum_i\alpha_ik（x_i中，x）=\mathrm{常数}。注意，如果k（x，y）变小为y的增长进一步远离的x，在求和的每一项测量测试点x的接近程度的相应数据基点x_i的程度。以这种方式，内核上面的总和可以被用于测量各个测试点的对数据点始发于一个或另一个集合中的要被鉴别的相对接近程度。注意一个事实，即设定点的x映射到任何超平面可以相当卷积的结果，使集未在原始空间凸出于各之间复杂得多歧视。
# 对偶问题
对偶问题 　每一个线性规划问题都伴随有另一个线性规划问题，称为对偶问题。原来的线性规划问题则称为原始线性规划问题，简称原始问题。对偶问题有许多重要的特征，它的变量能提供关于原始问题最优解的许多重要资料，有助于原始问题的求解和分析。对偶问题与原始问题之间存在着下列关系：
1. 目标函数对原始问题是极大化，对对偶问题则是极小化。
2. 原始问题目标函数中的收益系数是对偶问题约束不等式中的右端常数，而原始问题约束不等式中的右端常数则是对偶问题中目标函数的收益系数。
3. 原始问题和对偶问题的约束不等式的符号方向相反。
4. 原始问题约束不等式系数矩阵转置后即为对偶问题的约束不等式的系数矩阵。
5.  原始问题的约束方程数对应于对偶问题的变量数，而原始问题的变量数对应于对偶问题的约束方程数。
6. 对偶问题的对偶问题是原始问题，这一性质被称为原始和对偶问题的对称性。

# 凸优化
凸优化问题是指需要最小化的函数（代价函数）是凸函数，而且定义域为凸集的问题。